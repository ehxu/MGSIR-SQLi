# src/features/mgsir/extractor_enhanced.py
import os
import pickle
import numpy as np
import pandas as pd
import csv
from src.features.mgsir.hdcan import *
from src.features.mgsir.mgsif import *
from pathlib import Path


def get_ablation_features(mode):
    """
    æ ¹æ®æ¶ˆèæ¨¡å¼åç§°ï¼Œè¿”å›å¯¹åº”çš„ç‰¹å¾åˆ—ååˆ—è¡¨
    
    æ¨¡å¼è¯´æ˜ï¼š
    - L1/L2/L3/L4: å•ç‹¬æŸä¸€å±‚
    - L1_L2/L1_L2_L3: ç´¯åŠ å±‚
    - No_L1/No_L2/No_L3/No_L4: å‡å»æŸä¸€å±‚
    - Full: æ‰€æœ‰å±‚
    """
    L1, L2, L3, L4 = LEVEL_1_KEYS, LEVEL_2_KEYS, LEVEL_3_KEYS, LEVEL_4_KEYS
    
    # æ¨¡å¼æ˜ å°„è¡¨
    mode_map = {
        # å•ç‹¬å±‚
        "L1": L1, "L1_only": L1,
        "L2": L2, "L2_only": L2,
        "L3": L3, "L3_only": L3,
        "L4": L4, "L4_only": L4,
        
        # ç´¯åŠ å±‚
        "L1_L2": L1 + L2,
        "L1_L2_L3": L1 + L2 + L3,
        
        # å‡æ³•å±‚ (ä¿ç•™åˆ«åä»¥å…¼å®¹æ—§ä»£ç )
        "No_L1": L2 + L3 + L4,
        "No_L2": L1 + L3 + L4,
        "No_L3": L1 + L2 + L4,
        "No_L4": L1 + L2 + L3,  # åŒ L1_L2_L3
        
        # å®Œæ•´
        "Full": L1 + L2 + L3 + L4,
    }
    
    # è¿”å›å¯¹åº”æ¨¡å¼ï¼Œå¦‚æœæ¨¡å¼ä¸å­˜åœ¨åˆ™è¿”å›æ‰€æœ‰ç‰¹å¾
    return mode_map.get(mode, L1 + L2 + L3 + L4)


# é«˜çº§ç‰¹å¾æå–ä¸å¤„ç†
def process_dataframe_features(
    df,
    dataset_type="Train",
    feature_dir: Path | None = None,
    save_features=False,
    feature_cols=None,
):
    """
    è¾“å…¥: åªæœ‰ Query çš„ DataFrame
    è¾“å‡º: åŒ…å« Query, Query_preprocessed, qlen, wcount... ç­‰æ‰€æœ‰ç‰¹å¾çš„ DataFrame
    """
    print(
        f"\n[Processing] æ­£åœ¨å¤„ç† {dataset_type} é›†ç‰¹å¾ (Input Shape: {df.shape} | Mode: {len(feature_cols) if feature_cols else 'ALL'})..."
    )
    print("[Feature] æ‰§è¡Œé«˜çº§é¢„å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹...")
    if "Query" in df.columns:
        # 1. ç”Ÿæˆé¢„å¤„ç†åçš„ Query
        df["Query_preprocessed"] = df["Query"].apply(advanced_preprocess)

        # 2. æå–ç»“æ„åŒ–ç‰¹å¾ (extract_struct_features ä¼šæ ¹æ® Query/Query_preprocessed ç”Ÿæˆ qlen, wcount ç­‰)
        # è¿™ä¸€æ­¥ä¼šå°† DataFrame çš„åˆ—æ•°æ‰©å……
        # df = extract_struct_features(df)
        df = extract_struct_features(df, active_cols=feature_cols)

        print(
            f"[Processing] {dataset_type} ç‰¹å¾å·¥ç¨‹å¤„ç†å®Œæˆ (Output Shape: {df.shape})"
        )

        # # ä¿å­˜é€»è¾‘éœ€è°ƒæ•´æ–‡ä»¶åï¼Œé¿å…è¦†ç›–ï¼ˆå¯é€‰ï¼Œæˆ–è€…åœ¨å¤–éƒ¨æ§åˆ¶ç›®å½•ï¼‰
        # if save_features and feature_dir is not None:
        #      # ä¸ºäº†é¿å…æ¶ˆèå®éªŒè¦†ç›–åŸå§‹çš„å…¨é‡ç‰¹å¾æ–‡ä»¶ï¼Œæˆ‘ä»¬ä¸å»ºè®®åœ¨æ¶ˆèæ¨¡å¼ä¸‹è¦†ç›– feature_extracted_Train.csv
        #      # é™¤éæˆ‘ä»¬åœ¨å¤–éƒ¨æ”¹å˜äº† feature_dirã€‚
        #      # è¿™é‡Œå‡è®¾å¤–éƒ¨ä¼ å…¥çš„ feature_dir å·²ç»æ˜¯ç‹¬ç«‹çš„æ–‡ä»¶å¤¹ (e.g. checkpoints/ablation_L1/features)
        #     final_csv_path = os.path.join(feature_dir, f"feature_extracted_{dataset_type}.csv")
        #     df.to_csv(final_csv_path, index=False, encoding="utf-8") # ç®€åŒ–å†™æ³•
        #     print(f"[FeatureEng] ç‰¹å¾å·²ä¿å­˜: {final_csv_path}")

        # --- æ ‡å‡†å¤„ç†ï¼šå¦‚æœæ˜¯ Noneï¼Œå°±ä¸ä¿å­˜ ---
        if save_features and feature_dir is None:
            raise ValueError(
                "save_features=True ä½†æœªæä¾› feature_dirï¼Œè¯·ä¼ å…¥æœ‰æ•ˆç›®å½• Path"
            )

        # 3. ğŸ’¾ æ§åˆ¶æ˜¯å¦ä¿å­˜
        if save_features:
            # # --- åªæœ‰éœ€è¦ä¿å­˜æ—¶ï¼Œæ‰è®¡ç®—è·¯å¾„ ---
            # if output_dir is None:
            #     if os.path.isfile(feature_dir):
            #         # è¾“å…¥æ˜¯æ–‡ä»¶ (train.csv) -> å›é€€åˆ°çˆ¶ç›®å½•
            #         base_dir = os.path.dirname(feature_dir)
            #     else:
            #         # è¾“å…¥æ˜¯ç›®å½• -> ç›´æ¥ä½¿ç”¨
            #         base_dir = feature_dir

            #     # è‡ªåŠ¨åœ¨åŒçº§å»ºç«‹ features æ–‡ä»¶å¤¹
            #     output_dir = os.path.join(base_dir, "features")

            # # åˆ›å»ºç›®å½•
            # if not os.path.exists(output_dir):
            #     os.makedirs(output_dir)
            #     print(f"[Init] åˆ›å»ºç‰¹å¾è¾“å‡ºç›®å½•: {output_dir}")

            # ä¿å­˜æ–‡ä»¶
            final_csv_path = os.path.join(
                feature_dir, f"feature_extracted_{dataset_type}.csv"
            )
            # df.to_csv(final_csv_path, index=False)
            df.to_csv(
                final_csv_path,
                index=False,
                encoding="utf-8",
                quoting=csv.QUOTE_MINIMAL,
                quotechar='"',
                escapechar="\\",
                doublequote=True,
            )
            print(f"[FeatureEng] âœ… æˆåŠŸï¼ç‰¹å¾æ–‡ä»¶å·²ä¿å­˜ => {final_csv_path}")

        else:
            print(
                "[FeatureEng] â© save_features=Falseï¼Œè·³è¿‡æ–‡ä»¶ä¿å­˜ï¼Œä»…è¿”å› DataFrameã€‚"
            )
    else:
        raise ValueError("æ•°æ®ä¸­ç¼ºå°‘ 'Query' åˆ—")
    return df


# main ä½¿ç”¨
def prepare_datasets_from_files_enhanced(
    train_df_raw, test_df_raw, feature_name, base_dir, ablation_mode="Full"
):

    if ablation_mode == "Full":
        target_cols = get_ablation_features(ablation_mode)
        print(f"\n[Info] Using FULL feature mode.")
        print(f"[Info] Final feature list ({len(target_cols)}): {target_cols}")
    else:
        target_cols = get_ablation_features(ablation_mode)
        print(f"\n[Ablation] Mode: {ablation_mode}")
        print(f"[Ablation] Activated features ({len(target_cols)}): {target_cols}")

    # === Step 1: åˆ†åˆ«å¯¹ è®­ç»ƒé›† å’Œ æµ‹è¯•é›† åšç‰¹å¾å·¥ç¨‹ ===
    # è¿™é‡ŒåŸæ¥çš„ train_df_raw(1åˆ—) ä¼šå˜æˆ x_train_full(åå‡ åˆ—)
    # <class 'pandas.core.frame.DataFrame'>
    # Index(['Query', 'Label', 'Query_preprocessed', 'qlen', 'wcount', 'sq', 'dq', 'puncts', 'comments', 'spaces', 'logic', 'arith', 'hexnum', 'alpha', 'sqlkw', 'sqlfunc'], dtype='object')
    # 2. ä¼ é€’ target_cols ç»™å¤„ç†å‡½æ•°
    train_full = process_dataframe_features(
        train_df_raw.copy(),
        "Train",
        base_dir / "features",
        save_features=True,
        feature_cols=target_cols,
    )
    test_full = process_dataframe_features(
        test_df_raw.copy(),
        "Val",
        base_dir / "features",
        save_features=True,
        feature_cols=target_cols,
    )

    # <class 'pandas.core.frame.DataFrame'>
    X_train = train_full.drop(["Label"], axis=1)
    # <class 'numpy.ndarray'>
    y_train = train_full["Label"].values
    X_test = test_full.drop(["Label"], axis=1)
    y_test = test_full["Label"].values

    X_train_num_scaled, X_test_num_scaled, scaler_num = (
        standardize_and_combine_features(X_train, X_test, target_cols)
    )

    train_feat = X_train_num_scaled
    test_feat = X_test_num_scaled

    scaler_num_path = os.path.join(
        base_dir / "scaler", f"scaler_for_numeric_{feature_name}.pkl"
    )

    with open(scaler_num_path, "wb") as f:
        pickle.dump(scaler_num, f)

    return {
        "x_train": X_train,  # åŸå§‹è®­ç»ƒé›†ç‰¹å¾ (DataFrame)
        "x_test": X_test,  # åŸå§‹éªŒè¯é›†ç‰¹å¾ (DataFrame)
        "y_train": y_train,  # è®­ç»ƒé›†æ ‡ç­¾
        "y_test": y_test,  # éªŒè¯é›†æ ‡ç­¾
        "num_features": (train_feat, test_feat),  # æ ‡å‡†åŒ–åçš„æ•°å€¼ç‰¹å¾çŸ©é˜µ
        # "num_features": (X_train_num_scaled, X_test_num_scaled),
    }
